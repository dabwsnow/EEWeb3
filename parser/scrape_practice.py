import requests
from bs4 import BeautifulSoup
import sys
import os
import re
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.database import SessionLocal
from app import models
from tqdm import tqdm
import time

# URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
PRACTICE_URLS = {
    'inf02': 'https://ee-informatyk.pl/ee08-inf02/praktyka/',
    'inf03': 'https://ee-informatyk.pl/inf03-ee09/praktyka/',
    'inf04': 'https://ee-informatyk.pl/inf04/praktyka/',
}

# –ú–∞–ø–ø–∏–Ω–≥ –ø—Ä–æ—Ñ–∏–ª–µ–π –Ω–∞ –º–æ–¥–µ–ª–∏
PRACTICE_MODELS = {
    'inf02': models.PracticeINF02,
    'inf03': models.PracticeINF03,
    'inf04': models.PracticeINF04,
}

def download_file(url, save_path):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª"""
    try:
        response = requests.get(url, timeout=30, stream=True)
        response.raise_for_status()
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"      ‚úÖ –°–∫–∞—á–∞–Ω–æ: {save_path}")
        return True
    except Exception as e:
        print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

def parse_practice_page(base_url, profile_key):
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —ç–∫–∑–∞–º–µ–Ω–∞–º–∏"""
    print(f"\nüîç –ü–∞—Ä—Å–∏–Ω–≥: {base_url}")
    
    try:
        response = requests.get(base_url, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≥–æ–¥–∞
        year_select = soup.find('select', {'name': 'rok'})
        if not year_select:
            print("  ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä –≥–æ–¥–∞")
            return []
        
        years = [opt['value'] for opt in year_select.find_all('option') if opt['value'] != 'all']
        print(f"  üìÖ –ù–∞–π–¥–µ–Ω–æ –ª–µ—Ç: {len(years)}")
        
        all_practices = []
        
        for year in years:
            print(f"\n  üìÖ –ü–∞—Ä—Å–∏–Ω–≥ –≥–æ–¥–∞: {year}")
            
            # –ó–∞–ø—Ä–æ—Å —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –≥–æ–¥—É
            year_url = f"{base_url}?rok={year}"
            year_response = requests.get(year_url, timeout=15)
            year_soup = BeautifulSoup(year_response.text, "html.parser")
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è
            practice_items = year_soup.find_all('div', class_='practice-list--one')
            print(f"    üìù –ù–∞–π–¥–µ–Ω–æ –∑–∞–¥–∞–Ω–∏–π: {len(practice_items)}")
            
            for item in practice_items:
                try:
                    # –î–∞—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä "2025 - Stycze≈Ñ")
                    date_elem = item.find('div', class_='practice-list--one--date')
                    date = date_elem.find('h3').text.strip() if date_elem else None
                    
                    # –ö–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä "INF.03-12-25.01-SG")
                    code_elem = item.find('div', class_='practice-list--one--id')
                    code = code_elem.find('h3').text.strip() if code_elem else None
                    
                    if not date or not code:
                        continue
                    
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥ –∏–∑ –∫–æ–¥–∞
                    # INF.03-12-25.01-SG ‚Üí –±–µ—Ä–µ–º "25" ‚Üí 2025
                    # INF.04-03-23.06-SG ‚Üí –±–µ—Ä–µ–º "23" ‚Üí 2023
                    try:
                        parts = code.split('-')
                        if len(parts) >= 3:
                            # –ë–µ—Ä–µ–º —Ç—Ä–µ—Ç—å—é —á–∞—Å—Ç—å –∏ –ø–µ—Ä–≤—ã–µ 2 —Å–∏–º–≤–æ–ª–∞
                            year_part = parts[2].split('.')[0]  # "25" –∏–ª–∏ "23"
                            parsed_year = int(year_part)
                            
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–æ–ª–Ω—ã–π –≥–æ–¥ (14-99 ‚Üí 2014-2099)
                            if parsed_year >= 14:
                                parsed_year = 2000 + parsed_year
                            else:
                                # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ 14, —ç—Ç–æ 2000-2013 (–≤—Ä—è–¥ –ª–∏ –±—É–¥–µ—Ç)
                                parsed_year = 2000 + parsed_year
                        else:
                            parsed_year = int(year)  # fallback
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–æ–¥–∞ –∏–∑ –∫–æ–¥–∞: {e}")
                        parsed_year = int(year)  # fallback
                    
                    print(f"\n    üìã {code} ({date}) - –ì–æ–¥: {parsed_year}")
                    
                    # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏
                    links = item.find('ul', class_='practice-list--one--links')
                    if not links:
                        continue
                    
                    arkusz_url = None
                    pliki_url = None
                    rozwiazanie_url = None
                    
                    for link in links.find_all('a'):
                        href = link.get('href', '')
                        text = link.text.strip()
                        
                        if 'Arkusz' in text and 'RozwiƒÖzanie' not in text:
                            # –°—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∞—Ä–∫—É—à–µ–º
                            arkusz_page_url = f"https://ee-informatyk.pl{href}"
                            
                            # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞—Ä–∫—É—à–∞ –∏ –∏—â–µ–º PDF
                            try:
                                arkusz_response = requests.get(arkusz_page_url, timeout=15)
                                arkusz_soup = BeautifulSoup(arkusz_response.text, "html.parser")
                                
                                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ PDF
                                pdf_link = arkusz_soup.find('a', href=re.compile(r'\.pdf$'))
                                if pdf_link:
                                    arkusz_url = f"https://ee-informatyk.pl{pdf_link['href']}"
                                    print(f"      üìÑ Arkusz PDF: {arkusz_url}")
                            except Exception as e:
                                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∞—Ä–∫—É—à–∞: {e}")
                        
                        elif 'Pobierz Pliki' in text:
                            pliki_url = f"https://ee-informatyk.pl{href}"
                            print(f"      üì¶ Pliki: {pliki_url}")
                        
                        elif 'Pobierz RozwiƒÖzanie' in text:
                            rozwiazanie_url = f"https://ee-informatyk.pl{href}"
                            print(f"      ‚úÖ RozwiƒÖzanie: {rozwiazanie_url}")
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø (–ø–æ –∏–∫–æ–Ω–∫–∞–º)
                    icons = item.find_all('h3', {'data-wenk': True})
                    practice_type = []
                    for icon in icons:
                        wenk_text = icon.get('data-wenk', '').lower()
                        if 'php' in wenk_text:
                            practice_type.append('PHP')
                        if 'baza danych' in wenk_text or 'database' in wenk_text:
                            practice_type.append('Database')
                    
                    practice_data = {
                        'code': code,
                        'date': date,
                        'year': parsed_year,  # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–µ–º parsed_year
                        'type': ', '.join(practice_type) if practice_type else 'Standard',
                        'arkusz_url': arkusz_url,
                        'pliki_url': pliki_url,
                        'rozwiazanie_url': rozwiazanie_url,
                    }
                    
                    all_practices.append(practice_data)
                
                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –≥–æ–¥–∞–º–∏
        
        return all_practices
    
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return []
    
def scrape_profile_practice(profile_key, base_url):
    """–ü–∞—Ä—Å–∏—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è –¥–ª—è –ø—Ä–æ—Ñ–∏–ª—è"""
    model = PRACTICE_MODELS.get(profile_key)
    
    if not model:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å: {profile_key}")
        return
    
    print(f"\n{'='*60}")
    print(f"üìö –ü—Ä–∞–∫—Ç–∏–∫–∞ {profile_key.upper()}")
    print(f"{'='*60}")
    
    practices_data = parse_practice_page(base_url, profile_key)
    
    print(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(practices_data)}")
    
    db = SessionLocal()
    added = 0
    skipped = 0
    
    try:
        for p_data in tqdm(practices_data, desc="  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"):
            existing = db.query(model).filter(
                model.code == p_data['code']
            ).first()
            
            if existing:
                skipped += 1
                continue
            
            practice = model(
                code=p_data['code'],
                date=p_data['date'],
                year=int(p_data['year']),
                type=p_data['type'],
                arkusz_url=p_data['arkusz_url'],
                pliki_url=p_data['pliki_url'],
                rozwiazanie_url=p_data['rozwiazanie_url'],
                downloaded=0
            )
            
            db.add(practice)
            added += 1
        
        db.commit()
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   üìù –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {added}")
        print(f"   ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}\n")
    
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
    finally:
        db.close()

def main():
    print("\nüöÄ –°–¢–ê–†–¢ –ü–ê–†–°–ï–†–ê –ü–†–ê–ö–¢–ò–ö–ò\n")
    
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –ø—Ä–æ—Ñ–∏–ª–∏
    for profile_key, base_url in PRACTICE_URLS.items():
        try:
            scrape_profile_practice(profile_key, base_url)
            time.sleep(2)
        except Exception as e:
            print(f"‚ùå {profile_key}: {e}")
            continue
    
    print("üéâ –ì–û–¢–û–í–û!\n")

if __name__ == "__main__":
    main()